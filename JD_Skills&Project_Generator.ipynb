{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kUeif8qGeFah"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " Import libraries\n",
        "import os\n",
        "import google.generativeai as genai\n"
      ],
      "metadata": {
        "id": "KveReOBrF7TO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ STEP 1: Set your API key here"
      ],
      "metadata": {
        "id": "1awj7QYjKapX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"  # ðŸ”‘ Replace this with your own Gemini API key"
      ],
      "metadata": {
        "id": "Mxx_RQ_kJvQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "B9TP-2DyfdVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "0f21861d-a4f3-4094-b195-ad78e981f445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These projects build upon each other in complexity, offering a gradual introduction to Java, AWS, and microservices.  They focus on practical skills rather than overly complex architectures.\n",
            "\n",
            "**Project 1: Simple Book Inventory Service (Single Microservice)**\n",
            "\n",
            "* **Concept:** A single microservice that manages a simple inventory of books. Users can add, update, delete, and retrieve book information.  This focuses on basic microservice principles without the complexity of inter-service communication.\n",
            "* **Java:** Uses Spring Boot for ease of development and deployment.  Basic CRUD (Create, Read, Update, Delete) operations on an in-memory data store (like a `HashMap` initially, then upgrade to an in-memory database like H2).\n",
            "* **AWS:** Deploy the single microservice to AWS Elastic Beanstalk or AWS Lambda (for a serverless approach).  This introduces basic deployment and infrastructure management. No complex networking or databases are required at this stage.\n",
            "* **Microservices:** The project demonstrates the fundamental building block of a microservice: a single, self-contained application with a specific function.\n",
            "* **Learning Outcomes:** Basic Spring Boot, simple REST API creation, AWS deployment basics.\n",
            "\n",
            "**Project 2: Book Inventory & User Service (Two Microservices)**\n",
            "\n",
            "* **Concept:** Extend Project 1 by adding a separate user service.  The book inventory service can now only be accessed by authenticated users.\n",
            "* **Java:** Two Spring Boot applications. The user service manages user registration and authentication (consider a simple in-memory authentication for simplicity).  The book inventory service will interact with the user service via REST API calls to authenticate users.\n",
            "* **AWS:**  Deploy both microservices to separate Elastic Beanstalk environments or Lambda functions.  Introduce a basic load balancer (Application Load Balancer â€“ ALB) for the book inventory service to handle increased traffic (though not strictly necessary for a small project).\n",
            "* **Microservices:** This project introduces inter-service communication via REST APIs.  You'll learn about handling errors and managing dependencies between services.\n",
            "* **Learning Outcomes:** Inter-service communication, REST API design, basic load balancing, error handling in microservices.\n",
            "\n",
            "\n",
            "**Project 3: Book Inventory, User Service, and Recommendation Service (Three Microservices with Database)**\n",
            "\n",
            "* **Concept:** Add a recommendation service that suggests books based on user history (you can implement simple recommendation logic).  Persist data to a database.\n",
            "* **Java:** Three Spring Boot applications.  The recommendation service will interact with both the user and book inventory services.\n",
            "* **AWS:** Deploy all three microservices to separate Elastic Beanstalk environments or Lambda functions.  Use Amazon RDS (e.g., PostgreSQL or MySQL) as a persistent database.  Implement service discovery (e.g., using AWS Service Discovery) to allow the services to dynamically find each other.  Consider using API Gateway to manage API requests.\n",
            "* **Microservices:**  This project introduces persistent data storage, more complex inter-service communication, and service discovery.  You will learn about data consistency and handling failures across multiple services.\n",
            "* **Learning Outcomes:** Database integration, service discovery, API Gateway usage, more advanced error handling and resilience.\n",
            "\n",
            "\n",
            "Remember to start small, focus on one aspect at a time, and gradually increase complexity.  Each project builds on the previous one, allowing you to progressively learn the core concepts of Java, AWS, and microservices.  There are numerous tutorials and resources online for each technology involved.  Don't be afraid to experiment and learn from your mistakes.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”‘ Check and warn if API key is missing\n",
        "if \"GEMINI_API_KEY\" not in os.environ:\n",
        "    print(\"ðŸš¨ Please set your Gemini API Key using the following code block before continuing:\")\n",
        "    print('os.environ[\"GEMINI_API_KEY\"] = \"\"GEMINI_API_KEY\"\"')\n",
        "else:\n",
        "    # âœ… Configure the Gemini client with your API key\n",
        "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "    # ðŸ§  Load the model\n",
        "    model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash-latest\")\n",
        "\n",
        "    # âœ¨ Generate a response\n",
        "    response = model.generate_content(\"Suggest 3 beginner projects using Java, AWS, microservices.\")\n",
        "\n",
        "    # ðŸ“¤ Display the result\n",
        "    print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "rZEU-YBSfldQ",
        "outputId": "93ffc265-9690-4fa8-8b73-6bf7660b2986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MY6Mq3Vvh4CA"
      },
      "outputs": [],
      "source": [
        "selected_skill_level = \"Intermediate\"  # Change to \"Beginner\" or \"Advanced\" as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "id": "Nm8LzMlZiLau",
        "outputId": "c9968086-866b-4470-cd3b-e6740fcbdfc9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Data Engineer Project Ideas for Students/Freshers\n\n**1. Skills Required:** SQL, Python, Spark, Redshift, S3, Data pipeline orchestration (Airflow), ETL/ELT design, AWS tools (Glue, Athena), Data warehousing concepts.\n\n\n**Project Idea 1:  Dynamically Generated Sales Forecasting Dashboard using Amazon S3, Redshift, and Airflow.**\n\n**Concept:**  Build an automated sales forecasting dashboard that pulls data from various S3 buckets (simulated sales data across different regions and product categories).  The pipeline, orchestrated by Airflow, will perform ETL (extract, transform, load) operations.  Data is cleaned and transformed using Spark and Python, then loaded into Redshift.  Finally, a web dashboard (using a simple framework like Streamlit or Plotly Dash) displays interactive visualizations of forecasted sales, allowing users to filter by region, product, and time period.  The forecasting model itself could be a simple time series model (like ARIMA) implemented in Python, showcasing basic predictive capabilities.  The system should demonstrate the dynamic nature of data warehousing by updating the dashboard on a daily or weekly schedule.\n\n**Time to Complete:** 6-8 weeks\n\n**Learning Resources:**\n*   **Airflow Tutorial:**  YouTube channels dedicated to Airflow (search for \"Airflow Tutorial for Beginners\").\n*   **Streamlit/Plotly Dash:**  Official documentation and tutorials on Streamlit and Plotly Dash websites.\n*   **AWS Glue and Athena:** AWS documentation and training on Glue and Athena (AWS Skill Builder).\n\n\n**Project Idea 2:  Near Real-Time Social Media Sentiment Analysis Pipeline for E-commerce Product Reviews.**\n\n**Concept:**  Design and build a data pipeline that ingests social media data (e.g., Twitter or Reddit using relevant APIs â€“ consider using a simplified dataset to avoid rate limits initially).  Utilize Python and Spark for cleaning, pre-processing and sentiment analysis (using libraries like NLTK or Transformers).  The pipeline should be designed for near real-time processing, using technologies like Apache Kafka for message queuing (or a simplified equivalent) and Airflow for scheduling.  Results (aggregated sentiment scores per product) are stored in Redshift and visualized using a dashboard (Streamlit/Plotly Dash), enabling rapid identification of trending opinions about specific products. This project focuses on the real-time aspect and the challenges of handling semi-structured data.\n\n**Time to Complete:** 8-10 weeks\n\n**Learning Resources:**\n*   **Apache Kafka:**  Confluent Kafka documentation and tutorials.\n*   **Sentiment Analysis with Python:**  Numerous tutorials available on YouTube and Coursera (search for \"Sentiment Analysis Python NLTK\").\n*   **AWS services for real-time data processing:** AWS documentation for Kinesis, Lambda and other relevant services.\n\n\n**Project Idea 3:  A Comparative Analysis of Different Data Warehousing Approaches on a Public Dataset.**\n\n**Concept:** This project explores different approaches to data warehousing.  Choose a publicly available, sizeable dataset (e.g., from Kaggle).  Design and implement ETL pipelines using different AWS services (Glue vs. custom Spark jobs).  Load the data into Redshift and a different data warehouse service (e.g., Snowflake, if accessible).  Perform comparative analysis:  compare the performance (load times, query speeds), scalability, and cost-effectiveness of each approach.  Document your findings in a comprehensive report, showcasing the trade-offs of each methodology and your understanding of data warehousing principles. This emphasizes practical experimentation and critical analysis.\n\n**Time to Complete:** 6-8 weeks\n\n**Learning Resources:**\n*   **AWS Glue vs. Spark:**  AWS documentation and comparative articles comparing Glue and Spark for ETL.\n*   **Snowflake (or alternative) Documentation:**  Official documentation of the chosen alternative data warehouse.\n*   **Data Warehousing Concepts:**  Coursera courses on data warehousing.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# âœ… Paste the Job Description here\n",
        "job_description = \"\"\"\n",
        "Amazon is hiring a Data Engineer to build and optimize data pipelines. The role involves handling petabyte-scale datasets to support analytics and ML models.\n",
        "\n",
        "Required Skills:\n",
        "\n",
        "SQL, Python, Spark\n",
        "\n",
        "Redshift, S3\n",
        "\n",
        "Data pipeline orchestration (e.g., Airflow)\n",
        "\n",
        "ETL/ELT design\n",
        "\n",
        "AWS tools (Glue, Athena)\n",
        "\n",
        "Data warehousing concepts\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# âœ… Prompt with skill level\n",
        "prompt = f\"\"\"\n",
        "You are an AI career mentor. Your job is to produce **unique, non-generic** project ideas for students or freshers based on the job description below.\n",
        "\n",
        "Steps:\n",
        "1. Extract and list the **Skills required for the job description**.\n",
        "2. Generate **three distinct project ideas** that:\n",
        "   - Align with the extracted skills.\n",
        "   - Are **creative and not tutorial-based**.\n",
        "   - Mention **minimum time to complete (in weeks)**.\n",
        "   - Are appropriate for a **{selected_skill_level}** user.\n",
        "   - Include **2â€“3 useful learning resources** (YouTube/Coursera/Microsoft Learn).\n",
        "\n",
        "Job Description:\n",
        "{job_description}\n",
        "\"\"\"\n",
        "\n",
        "# âœ… Generate with Gemini\n",
        "model = genai.GenerativeModel('models/gemini-1.5-flash-latest') # Changed to a valid model name\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# âœ… Display as Markdown\n",
        "from IPython.display import Markdown\n",
        "display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "widgets": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}