{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kUeif8qGeFah"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9TP-2DyfdVo"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# ✅ Set your Gemini API key here\n",
        "genai.configure(api_key=\"AIzaSyBYU6GLsaxjp1tCcegQuboz57ngJJKBo4Q\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "lo4oufXFf7-I",
        "outputId": "71195406-0622-4488-b312-dec6ee4a0129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "These projects leverage Java, AWS, and microservices, scaling in complexity.  They focus on practical learning rather than overly ambitious scope.\n",
            "\n",
            "**Project 1: Simple Book Inventory Service**\n",
            "\n",
            "* **Microservices:** Two:\n",
            "    * **Inventory Service:** Manages a list of books (ID, title, author, quantity).  Uses an in-memory data store (like an ArrayList initially, then upgrade to an in-memory DB like H2 for persistence) for simplicity.  Exposes a REST API (using Spring Boot) for CRUD operations (Create, Read, Update, Delete).\n",
            "    * **Recommendation Service:**  (Optional, but good for learning)  A very basic service that suggests books based on a simple algorithm (e.g., randomly selecting books from the inventory).  Also exposes a REST API.\n",
            "\n",
            "* **AWS:**\n",
            "    * **Amazon EC2:** Deploy each microservice to a separate EC2 instance (t2.micro is sufficient for learning). This provides a basic understanding of deployment.  Alternatively, use AWS Elastic Beanstalk for easier deployment.\n",
            "    * **Simple Load Balancer (Optional):** If you add the recommendation service and want to load balance requests across both, you can introduce a simple load balancer in front of your instances.\n",
            "\n",
            "* **Java:** Spring Boot is ideal for creating the REST APIs quickly.  You’ll learn basic Spring concepts, RESTful design, and dependency injection.\n",
            "\n",
            "\n",
            "**Project 2:  Basic E-commerce Product Catalog**\n",
            "\n",
            "* **Microservices:** Three:\n",
            "    * **Catalog Service:**  Similar to the Book Inventory, but handles product information (ID, name, description, price, images).  Persist data in a simple database (e.g., Amazon RDS with PostgreSQL or MySQL).\n",
            "    * **Pricing Service:**  Calculates the final price of a product (including discounts or taxes).  This could interact with a separate database for pricing rules.\n",
            "    * **Image Service:** (Optional, but adds complexity) Stores and retrieves product images. Uses Amazon S3 for image storage.\n",
            "\n",
            "\n",
            "* **AWS:**\n",
            "    * **Amazon EC2 or Elastic Beanstalk:** For deploying your microservices.\n",
            "    * **Amazon RDS:** For persistent data storage.\n",
            "    * **Amazon S3:** (If including the Image Service) For storing images.\n",
            "\n",
            "\n",
            "* **Java:** Spring Boot remains excellent. You’ll build on your knowledge from Project 1, adding database interaction (using Spring Data JPA or similar) and potentially asynchronous communication between services (e.g., using a message queue like Amazon SQS, though not strictly necessary at this beginner stage).\n",
            "\n",
            "\n",
            "\n",
            "**Project 3:  Simplified User Authentication and Authorization**\n",
            "\n",
            "* **Microservices:** Two:\n",
            "    * **Authentication Service:** Handles user registration and login.  Stores user credentials securely (e.g., using password hashing).  Uses JWT (JSON Web Tokens) for authentication.\n",
            "    * **Authorization Service:**  Verifies if a user has permission to access specific resources.  Could use a simple role-based access control (RBAC) system.\n",
            "\n",
            "* **AWS:**\n",
            "    * **Amazon Cognito (Recommended):**  Leverage Cognito for user management and authentication.  This simplifies the task significantly and is a good practice for production systems.  Alternatively, you could manage users in your own database, but Cognito is much easier to get started with.\n",
            "    * **API Gateway (Optional):**  Put an API Gateway in front of your services to manage requests, authentication, and authorization.\n",
            "\n",
            "* **Java:** Spring Boot with Spring Security for securing your APIs and handling authentication/authorization.  You'll learn about security best practices and JWT.\n",
            "\n",
            "\n",
            "Remember to start small, focus on one aspect at a time, and gradually increase complexity.  For each project, don't feel pressured to implement all optional components immediately; get the core functionality working first.  Using tutorials and online documentation will be crucial.  Good luck!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use the correct model: 'models/gemini-1.5-pro' or 'models/gemini-pro'\n",
        "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash-latest\")  # Changed to a valid model name\n",
        "\n",
        "response = model.generate_content(\"Suggest 3 beginner projects using Java, AWS, microservices.\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "rZEU-YBSfldQ",
        "outputId": "846d49ae-da20-4b16-eeb6-7b45bd5f4739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MY6Mq3Vvh4CA"
      },
      "outputs": [],
      "source": [
        "selected_skill_level = \"Intermediate\"  # Change to \"Beginner\" or \"Advanced\" as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "Nm8LzMlZiLau",
        "outputId": "7cf08d74-6a81-4118-e77c-1c18cd93a6a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Data Engineer Project Ideas (Intermediate Level)\n\n**1. Skills Required:** SQL, Python, Spark, Redshift, S3, Data pipeline orchestration (Airflow), ETL/ELT design, AWS tools (Glue, Athena), Data warehousing concepts.\n\n\n**Project Idea 1:  Smart City Traffic Optimization using Real-time Data Feeds.**\n\n**Concept:**  Develop a data pipeline that ingests simulated real-time traffic data (speed, density, accidents) from multiple sources (APIs, simulated sensor data).  Process this data using Spark to identify traffic bottlenecks and predict congestion patterns.  Store processed data in Redshift for analysis and visualization.  Use Airflow to orchestrate the pipeline, ensuring data is processed and loaded into Redshift in near real-time.  Finally, create a basic dashboard (using tools like Tableau or a simple Python visualization library) showing key traffic metrics and predictions. This project simulates a real-world application of large-scale data processing for a common urban problem.\n\n**Time to Complete:** 6-8 weeks\n\n**Learning Resources:**\n\n* **Apache Spark Fundamentals on Coursera:**  Covers the basics of Spark for data processing.\n* **Building a Data Pipeline with Apache Airflow (YouTube):** Numerous tutorials are available on building Airflow DAGs.\n* **AWS Glue and Athena documentation:**  Understand the specifics of AWS services relevant to the project.\n\n\n**2. Skills Required:** SQL, Python, Spark, Redshift, S3, Data pipeline orchestration (Airflow), ETL/ELT design, AWS tools (Glue, Athena), Data warehousing concepts.\n\n**Project Idea 2:  E-commerce Sales Forecasting and Anomaly Detection.**\n\n**Concept:** Create a data pipeline that processes a large e-commerce dataset (publicly available datasets or simulated data).  This project will focus on forecasting future sales based on historical data.  Use techniques like time series analysis with ARIMA or Prophet (Python library) to build a predictive model.  Integrate this model into your pipeline using Spark for scalability.  Implement anomaly detection to identify unusual spikes or dips in sales that might require investigation.  The entire pipeline should be orchestrated using Airflow, storing results in Redshift for further analysis and reporting.\n\n**Time to Complete:** 8-10 weeks\n\n**Learning Resources:**\n\n* **Time Series Analysis in Python (YouTube):** Numerous videos cover various time series models.\n* **Prophet documentation (Facebook):** Learn how to use Prophet for forecasting.\n* **AWS Machine Learning Blog:** Find relevant articles on implementing ML models on AWS.\n\n\n**3. Skills Required:** SQL, Python, Spark, Redshift, S3, Data pipeline orchestration (Airflow), ETL/ELT design, AWS tools (Glue, Athena), Data warehousing concepts.\n\n**Project Idea 3:  A/B Testing Result Analysis Pipeline for a Simulated Website.**\n\n**Concept:** Design a data pipeline for analyzing A/B test results from a simulated website. Simulate user interactions and create a dataset containing user IDs, which version of the website they saw, and key metrics (e.g., click-through rate, conversion rate).  Use SQL and Python to process the data, calculating key statistics and performing statistical significance tests to determine which version performed better. The processed results should be loaded into Redshift.  Use Airflow to automate the data extraction, transformation, loading, and analysis, ensuring repeatability and scalability for future A/B tests.\n\n**Time to Complete:** 6-8 weeks\n\n\n**Learning Resources:**\n\n* **SQL for Data Analysis (Coursera):** Strengthen your SQL skills for data manipulation and analysis.\n* **Statistical Hypothesis Testing Tutorials (YouTube):**  Learn about A/B testing and statistical significance.\n* **Data Analysis with Python and Pandas (YouTube):** Utilize Pandas for efficient data manipulation and analysis.\n\n\nThese projects encourage creative problem-solving and practical application of skills, making them superior to simple tutorial reproductions.  Remember to focus on documenting your process and showcasing your results effectively.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ✅ Paste the Job Description here\n",
        "job_description = \"\"\"\n",
        "Amazon is hiring a Data Engineer to build and optimize data pipelines. The role involves handling petabyte-scale datasets to support analytics and ML models.\n",
        "\n",
        "Required Skills:\n",
        "\n",
        "SQL, Python, Spark\n",
        "\n",
        "Redshift, S3\n",
        "\n",
        "Data pipeline orchestration (e.g., Airflow)\n",
        "\n",
        "ETL/ELT design\n",
        "\n",
        "AWS tools (Glue, Athena)\n",
        "\n",
        "Data warehousing concepts\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ✅ Prompt with skill level\n",
        "prompt = f\"\"\"\n",
        "You are an AI career mentor. Your job is to produce **unique, non-generic** project ideas for students or freshers based on the job description below.\n",
        "\n",
        "Steps:\n",
        "1. Extract and list the **Skills required for the job description**.\n",
        "2. Generate **three distinct project ideas** that:\n",
        "   - Align with the extracted skills.\n",
        "   - Are **creative and not tutorial-based**.\n",
        "   - Mention **minimum time to complete (in weeks)**.\n",
        "   - Are appropriate for a **{selected_skill_level}** user.\n",
        "   - Include **2–3 useful learning resources** (YouTube/Coursera/Microsoft Learn).\n",
        "\n",
        "Job Description:\n",
        "{job_description}\n",
        "\"\"\"\n",
        "\n",
        "# ✅ Generate with Gemini\n",
        "model = genai.GenerativeModel('models/gemini-1.5-flash-latest') # Changed to a valid model name\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# ✅ Display as Markdown\n",
        "from IPython.display import Markdown\n",
        "display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "widgets": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}